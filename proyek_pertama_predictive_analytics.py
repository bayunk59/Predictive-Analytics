# -*- coding: utf-8 -*-
"""Proyek Pertama: Predictive Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kK9TZcCkz-ndg5Rg01aGiGwg2VW5dN6c

Predictive Analytics menggunakan Data Weather Type Classification

# Data Loading
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

# Load data
!pip install kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d nikhil7280/weather-type-classification
!unzip weather-type-classification.zip

cuaca = pd.read_csv('weather_classification_data.csv')
cuaca.head()

"""# Exploratory Data Analysis"""

cuaca.info()

"""## Mengubah Type data"""

# ubah data Weather Type menjadi numerik
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
cuaca['Weather Type'] = le.fit_transform(cuaca['Weather Type'])
cuaca.head()

"""Mengubah data Weather Type menjadi numerik karena fitur ini akan menjadi target prediksi kita
Mengubahnya menjadi numerik akan mempermudah pengambilan keputusan

0 = Cloudy
1 = Rainy
2 = Snowy
3 = Sunny
"""

# update data
cuaca.describe()

"""## Menangani Outliers

"""

#menampilkan data outlier

import numpy as np
import matplotlib.pyplot as plt

# Loop through numerical columns and create boxplots
for column in cuaca.select_dtypes(include=np.number).columns:
  plt.figure(figsize=(8, 6))
  sns.boxplot(x=cuaca[column])
  plt.title(f'Boxplot of {column}')
  plt.show()

# Menghitung Q1 dan Q3 untuk kolom numerik
for column in cuaca.select_dtypes(include=np.number).columns:
  Q1 = cuaca[column].quantile(0.25)
  Q3 = cuaca[column].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  outlier_count = len(cuaca[(cuaca[column] < lower_bound) | (cuaca[column] > upper_bound)])
  print(f"Jumlah outlier di kolom '{column}': {outlier_count}")

"""Berdasarkan jumlah outlier, jika dijumlahkan datanya ada 1806 atau 13,6% dari total 13200 data.
jika dihapus akan mengurangi data lumayan banyak.
Saya mengganti nilai di kolom tertentu yang berada di luar batas atas dan bawah dengan nilai median kolom tersebut agar tidak menghilangkan niali outliernya dan tidak membuang data
"""

import numpy as np

# Fungsi untuk mengganti outlier dengan median berdasarkan batas yang ditentukan
def replace_outliers(df, column, upper_limit, lower_limit=None):
    if lower_limit is None:
        lower_limit = df[column].min()  # Menggunakan nilai minimum kolom jika tidak ada batas bawah
    median_value = df[column].median()  # Menghitung median kolom
    df[column] = np.where((df[column] < lower_limit) | (df[column] > upper_limit), median_value, df[column])
    return df

# Menangani outlier pada setiap kolom sesuai batas yang sudah ditentukan
cuaca = replace_outliers(cuaca, 'Temperature', upper_limit=50)  # Mengganti suhu > 50Â°C dengan median
cuaca = replace_outliers(cuaca, 'Humidity', upper_limit=100)  # Mengganti kelembaban > 100% dengan median
cuaca = replace_outliers(cuaca, 'Wind Speed', upper_limit=25)  # Mengganti kecepatan angin > 25 m/s dengan median
cuaca = replace_outliers(cuaca, 'Precipitation (%)', upper_limit=100)  # Mengganti curah hujan > 100% dengan median
cuaca = replace_outliers(cuaca, 'Atmospheric Pressure', upper_limit=1060, lower_limit=970)  # Mengganti tekanan udara di luar 970-1060 hPa dengan median
cuaca = replace_outliers(cuaca, 'Visibility (km)', upper_limit=15)  # Mengganti visibilitas > 15 km dengan median

# Mengecek deskripsi ulang data setelah penggantian
print(cuaca.describe())

"""## Univariate Analysis"""

# bagi menjadi 2 fitur
numerical_features = ['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)', 'Atmospheric Pressure', 'UV Index', 'Visibility (km)', 'Weather Type']
categorical_features = ['Cloud Cover', 'Season', 'Location']

"""### Categorical Features"""

# Fitur CLoud Cover
feature = categorical_features[0]
count = cuaca[feature].value_counts()
percent = 100*cuaca[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

# Fitur Season
feature = categorical_features[1]
count = cuaca[feature].value_counts()
percent = 100*cuaca[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

# Fitur Location
feature = categorical_features[2]
count = cuaca[feature].value_counts()
percent = 100*cuaca[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""### Numerical Features"""

cuaca.hist(bins=50, figsize=(20,15))
plt.show()

"""## Multivariate Analysis

### Categorical Features
"""

cat_features = cuaca.select_dtypes(include='object').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y="Weather Type", kind="bar", dodge=False, height = 4, aspect = 3,  data=cuaca, palette="Set3")
  plt.title("Rata-rata 'Type Cuaca' Relatif terhadap - {}".format(col))

"""berdasarkan data grafik di atas:
1. Pada fitur 'Cloud Cover', ada perbedaan signifikan pada kategori clear yang menandakan adanya hubungan antara 'Cloud Cover' dengan 'Weather Type'
2. Pada fitur 'Season', rata-rata Tipe cuaca yang muncul hampir sama di kisaran 1,2 - 1,6 menandakan hubungan 'Season' dengan 'Weather Type' rendah
3. Pada fitur 'Location', rata-rata Tipe cuaca yang juga hampir mirip. Ini juga menandakan rendahnya hubungan antara fitur 'Location' dan 'Weather Type'


"""

# Ubah menjadi numerik
cuaca1 = cuaca.copy() # buat kopian data agar data asli tidak berubah
le = LabelEncoder()
for feature in ['Cloud Cover', 'Season', 'Location']:
  cuaca1[feature] = le.fit_transform(cuaca[feature])

cuaca1.head()

"""Untuk melihat hubungan fitur kategori dengan Weather Type terutama pada fitur 'Cloud COver', saya ubah menjadi data numerik dan melihat korelasinya"""

# Mengetahui skor korelasi
plt.figure(figsize=(10, 8))
correlation_matrix = cuaca1[['Cloud Cover', 'Season', 'Location', 'Weather Type']].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur kategorik", size=20)
plt.tight_layout()

"""Dari hasil cek korelasi di atas benar prediksi daya jika Cloud Cover memiliki korelasi -0,54 dengan Weather Type. Karena bernilai negatif, ini menandakan bahwa semakin tinggi tingkat awan (mendung), semakin buruk jenis cuaca yang diprediksi, dan sebaliknya. Hubungan ini berkebalikan dengan data dan akan kita abaikan

### Numerical Features
"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(cuaca, diag_kind = 'kde')

# Mengetahui skor korelasi
plt.figure(figsize=(10, 8))
correlation_matrix = cuaca[numerical_features].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)
plt.tight_layout()

"""Berdasarkan nilai korelasi di atas
- UV index menjadi fitur yang paling mempengaruhi Tipe Cuaca
- Temperature dan Visibilty adalah fitur yang tidak mempunya korelasi dengan tipe cuaca dan akan di hapus
"""

# Ada beberapa yang tidak memilik korelasi dengan Weather Type, maka kita hilangkan saja
cuaca.drop(['Temperature', 'Visibility (km)'], inplace=True, axis=1)
cuaca.head()

cuaca.info()

"""# Data Preparation

## Encoding FItur Kategori

Ubah data kategori menjadi numerik dengan teknik one-hot encoding
"""

from sklearn.preprocessing import  OneHotEncoder
cuaca = pd.concat([cuaca, pd.get_dummies(cuaca['Cloud Cover'], prefix='Cloud Cover')],axis=1)
cuaca = pd.concat([cuaca, pd.get_dummies(cuaca['Season'], prefix='Season')],axis=1)
cuaca = pd.concat([cuaca, pd.get_dummies(cuaca['Location'], prefix='Location')],axis=1)
cuaca.drop(['Cloud Cover','Season','Location'], axis=1, inplace=True)
cuaca.head()

"""## Train-Test-Split

Kita akan membagi dataset menjadi data latih (train) dan data uji (test), pada data ini saya akan membagi data menjadi 90:10.
jika ada 13200 data seharusnya 11880 data latih dan 1320 data uji
"""

# Membagi 90:10 (10% untuk data uji/test)
from sklearn.model_selection import train_test_split

X = cuaca.drop(['Weather Type'],axis =1)
y = cuaca['Weather Type']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

# cek jumlah sampel
print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""## Standarisasi

Selanjutnya akan kita standarisasi data numeriknya menggunakan teknik StandarScaler. Teknik ini mengurangkan nilai rata-rata kemudian membaginya dengan stranda deviasi untuk menggeser nilai distribusinya menjadi -1 sampai 1
"""

# Standarisasi data latih (train) dengan StandardCaler (utk numerik)
from sklearn.preprocessing import StandardScaler

numerical_features = ['Humidity', 'Wind Speed', 'Precipitation (%)','Atmospheric Pressure', 'UV Index']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

# mengecek nilai mean dan standar deviasi pada setelah proses standarisasi
X_train[numerical_features].describe().round(4)

"""# Model Deployment

Pada tahap permodelan ini saya akan menggunakan 3 model lalu memilih yang terbaik di antaranya
1. K-Nearest Neighbor (KNN)
2. Random Forest (RF)
3. Boosting Algorithm
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""## Model K-Nearest Neighbor (K-NN)

"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)

models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""Melatih dengan KNN

## Model Random Forest
"""

# Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor

# buat model prediksi
RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)

models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""## Model Boosting Algorithm"""

from sklearn.ensemble import AdaBoostRegressor

boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)
boosting.fit(X_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

"""# Evaluasi Model

Proses scaling fitur numerik pada data uji, hal ini dilakukan untuk menyamakan skala di data uji dengan data latih agar bisa di evaluasi
"""

# Proses Scalling
# Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

# Panggil mse
mse

"""Selanjutnya dilakukan evaluasi pada 3 model mengguanakan MSE (Mean Squared
Error),
lalu kita visualisasikan hasilnya dalam bentuk plot grafik
"""

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)
plt.xticks(rotation=45)

# melihat nilai akurasi dari tiap model
from sklearn.metrics import accuracy_score

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}

# Hitung akurasi masing-masing algoritma pada data test
for name, model in model_dict.items():
    y_pred = model.predict(X_test)
    # Konversi prediksi menjadi kelas (bulatkan ke bilangan bulat terdekat)
    y_pred_class = np.round(y_pred).astype(int)
    accuracy = accuracy_score(y_test, y_pred_class)
    print(f"Akurasi {name}: {accuracy:.4f}")

"""Setelah dibuat akurasinya dalam bentuk nilai, akurasi Random Forest menjadi yang terbesar dengan presentase 88,71%

Selanjutnya kita uji prediksinya menggunakan beberapa nilai dalam data
"""

# Uji data
prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""Berdasarkan nilai akurasinya dan prediksinya, permodelan yang paling mendekati dengan hasil aslinya adalah Random Forest"""