# -*- coding: utf-8 -*-
"""Proyek Pertama: Predictive Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kK9TZcCkz-ndg5Rg01aGiGwg2VW5dN6c

Predictive Analytics menggunakan Data Weather Type Classification

# Data Loading
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from sklearn import metrics
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

# Load data
!pip install kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d nikhil7280/weather-type-classification
!unzip weather-type-classification.zip

# Membuat dataset bernama cuaca
cuaca = pd.read_csv('weather_classification_data.csv')
cuaca.head()

"""# Exploratory Data Analysis"""

cuaca.info()

"""Data yang digunakan berasal dari kaggle dengan judul "Weather Type Classification" yang dapat di unduh [disini](https://www.kaggle.com/datasets/nikhil7280/weather-type-classification).

Variabel yang terdapat pada dataset adalah sebagai berikut:

- `Temperature` (numeric) : Temperatur suhu dalam celcius
- `Humidity` (numeric) : Presentase Kelembaban
- `Wind Speed` (numeric) : Kecepatan angin dalam kilometer/jam
- `Precipitation (%)` (numeric) : Presentase curah hujan
- `Cloud Cover` (categorical) : Deskripsi tutupan awan yang berisi clear, cloudy, overcast dan party cloudy
- `Atmospheric Pressure` (numeric) : Tekanan atmosfer dalam hPa
- `UV index` (numeric) : Indeks UX yang menunjukkan kekuatan radiasi UV
- `Season` (categorical) : Jenis musim mulai dari Autumn, Spring, Summer dan Winter
- `Visibility` (km) (numeric) : Jarak pandang dalam km
- `Location` (categorical) : Lokasi dimana data di ambil seperti coastal, inland dan muntain
- `Weather Type` (categorical) : Jenis cuaca yang berisi Cloudy, Rainy, Snowy dan Sunny (Target Klasifikasi)

Totalnya ada 11 variabel dengan jumlah 13200 data
"""

# Cek nilai duplikat pada data
duplicate_rows = cuaca[cuaca.duplicated()]
print("Jumlah baris duplikat:", duplicate_rows.shape[0])

"""Berdasarkan hasil pengecekan tidak ditemukan nilai duplikat"""

# Cek nilai kosong pada data
print(cuaca.isnull().sum())

"""Berdasarkan pengecekan juga, tidak ditemukan data yang kosong

## Univariate Analysis

Univariate Analysis adalah jenis analisis data yang memeriksa satu variabel saja. Tujuannya uuntuk menggambarkan data dan menemukan pola distribusi data

Sebelum mulai analysis kita bagi datanya menjadi 2 bagian, yakni `numerical_fitur` untuk data numerik dan `categorical_features` untuk data kategorik
"""

# bagi menjadi 2 fitur
numerical_features = ['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)', 'Atmospheric Pressure', 'UV Index', 'Visibility (km)']
categorical_features = ['Cloud Cover', 'Season', 'Location', 'Weather Type']

"""### Categorical Features"""

# Fitur CLoud Cover
feature = categorical_features[0]
count = cuaca[feature].value_counts()
percent = 100*cuaca[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Berdasarkan grafik pada fitur `Cloud Cover` di atas:
- `overcast` memiliki 6090 data
- `party cloud` memiliki 4560 data
- `clear` memiliki 2139 data
- `cloudy` memiliki 411 data
"""

# Fitur Season
feature = categorical_features[1]
count = cuaca[feature].value_counts()
percent = 100*cuaca[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Berdasarkan grafik pada fitur `Season` di atas:
- `winter` memiliki 5610 data
- `Spring` memiliki 2598 data
- `Autumn` memiliki 2500 data
- `Summer` memiliki 2492 data
"""

# Fitur Location
feature = categorical_features[2]
count = cuaca[feature].value_counts()
percent = 100*cuaca[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Berdasarkan grafik pada fitur `Location` di atas:
- `inland` memiliki 4816 data
- `mountain` memiliki 4813 data
- `coastal` memiliki 3571 data
"""

# Fitur Weather Type
feature = categorical_features[3]
count = cuaca[feature].value_counts()
percent = 100*cuaca[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Berdasarkan grafik pada fitur `Weather Type` di atas, nilai pada kolom ini terlihat seimbang dengan rincian sebagai berikut:
- `Rainy` memiliki 3300 data
- `Cluody` memiliki 3300 data
- `Sunny` memiliki 3300 data
- `Snowy` memiliki 3300 data

### Numerical Features
"""

cuaca.hist(bins=50, figsize=(20,15))
plt.show()

"""Berdasarkan grafik diatas, hampir semmua kolom skewnessnya mengarah ke kiri kecuali `Humidity`, `Precipitaion (%)` dan `Atmospheric Pressure`.

## Multivariate Analysis

Multivariate Analysis menunjukkan hubungan antara dua atau lebih variabel pada data. Multivariate Analysis yang menunjukkan hubungan antara dua variabel biasa disebut sebagai bivariate Analysis. Selanjutnya, kita akan melakukan analisis data pada fitur kategori dan numerik.

### Categorical Features
"""

for feature in categorical_features[:-1]:
  plt.figure(figsize=(10, 6))
  sns.countplot(x=feature, hue='Weather Type', data=cuaca)
  plt.title(f'Hubungan {feature} dengan Weather Type')
  plt.xlabel(feature)
  plt.ylabel('Jumlah Data')
  plt.legend(title='Weather Type')
  plt.show()

"""Berdasarkan grafik di atas didapatn:
-  Pada Fitur `Cloud Cover`
  1. Pada `party cloudy` jumlah `Coudy` menjadi yang terbanyak hampir mendekati 2000 data
  2. pada `Clear` semua nilainya diisi dengan tipe `Sunny`
  3. pada `overccast` nilia terendahnya adda pada `Sunny` dan yang terbanyak adalah `Rainy` dan `Snowy`
  4. pada `cludy`, hampr semua datanya rata

- Pada fitur `Season`
  1. pada `winter` jumlah `Snowy` menjadi yang paling besar melebihi 3000 data, sedangkan yang lain ada di bawah 1000 data
  2. pada `Spring`, `Summer` dan `Autumn` datanya hampir rata antara 500 - 1000 data kecuali pada data `Snowy` yang jumlahnya sangat sedikit

- Pada fitur `Location`
  1. pada `inland` dan `mountain` jumlah datanya hampir sama dengan data tertinggi ada pada `Snowy`
  2. Sedangkan pada `coastal`, data `Snowy` menjdai yang terendah dengan jumlah kurang dari 200 data

### Numerical Features
"""

sns.pairplot(cuaca, hue='Weather Type')
plt.show()

"""Berdasarkan visualisasi data diatas, tidak terlihat adanya hubungan yang signifikan antara fitur dengan target `Weather Type`

# Data Preparation

Data preparation merupakan tahapan penting dalam proses pengembangan model machine learning. Ini adalah tahap di mana kita melakukan proses transformasi pada data sehingga menjadi bentuk yang cocok untuk proses pemodelan.

## Menangani Outliers

Outlier adalah titik data yang secara signifikan berada di sebgaian data dalam kumpulan data. Outlier ini bisa muncul karena banyak faktor salah satunya adalah kesalahan pengamatan.
"""

#menampilkan data outlier
for column in cuaca.select_dtypes(include=np.number).columns:
  plt.figure(figsize=(8, 6))
  sns.boxplot(x=cuaca[column])
  plt.title(f'Boxplot of {column}')
  plt.show()

"""Berdasarkan boxplot diatas, ada 4 fitur yang memiliki outlier yakni fitur `Temperature`, `Wind Speed`, `Athmospheric Pressure` dan `Visibility (km)`

Outlier perlu dihapus untuk mendapatkan model yang bagus

"""

# Pilih yang numerik saja
numeric_cuaca = cuaca.select_dtypes(include=np.number)

Q1 = numeric_cuaca.quantile(0.25)
Q3 = numeric_cuaca.quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

cuaca = cuaca[~((numeric_cuaca < lower_bound) | (numeric_cuaca > upper_bound)).any(axis=1)]

"""Outlier telah dihapus"""

cuaca.shape

"""Jumlah data sekarang menjadi 11689 dari 13200 data

## Mengubah Type data
"""

# ubah semua data kategori menjadi numerik
le = LabelEncoder()
cuaca['Cloud Cover'] = le.fit_transform(cuaca['Cloud Cover'])
cuaca['Location'] = le.fit_transform(cuaca['Location'])
cuaca['Season'] = le.fit_transform(cuaca['Season'])
cuaca['Weather Type'] = le.fit_transform(cuaca['Weather Type'])
cuaca.head()

"""- Mengubah semua data kategorik menjadi numerik untuk mempermudah permodelan"""

# update data
cuaca.describe()

# cek data lagi
cuaca.info()

# Mengetahui skor korelasi
plt.figure(figsize=(10, 8))
correlation_matrix = cuaca.corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)
plt.tight_layout()

"""Berdasarkan nilai korelasi di atas
- `Temperature`, `Visibilty (km)`, dan `Location` adalah fitur yang mempunyai nilai korelasi paling kecil dengan target `Weather Type` dan akan di hapus

## Hapus Kolom dengan Korelasi Terendah

bagian ini adalah proses penghapusan fitur-fitur yang memiliki korelasi rendah terhadap variabel target dari dataset. Langkah ini diambil berdasarkan asumsi bahwa fitur dengan korelasi rendah tidak memberikan kontribusi signifikan terhadap prediksi yang dibuat oleh model.
"""

# Ada beberapa yang tidak memilik korelasi dengan Weather Type, maka dihilangkan
cuaca.drop(['Temperature', 'Visibility (km)', 'Location'], inplace=True, axis=1)
cuaca.head()

cuaca.info()

"""Penghapusan fitur `Temperature` ,`Visibilty (km)`, `Location` karena memiliki nilai korelasi yang rendah. Berdasarkan data terbaru, tersisa 8 kolom

## Train-Test-Split

Train-Test-Split adalah metode untuk membagi dataset menjadi data latih (train) dan data uji (test). Biasanya data akan dibagi dengan proporsi tertentu. Dalam kasus ini saya akan membagi data menjadi 90:10 dimana 90% untuk training dan 10% untuk testing
"""

# Membagi 90:10 (10% untuk data uji/test)
X = cuaca.drop(['Weather Type'],axis =1)
y = cuaca['Weather Type']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

# cek jumlah sampel
print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""Berdasarkan output diatas kita telah sukses melakukan proses Train-Test-Split, terlihat bahwa:
- Dataset train memiliki 10520 data
- Dataset test memiliki 1169 data

## Standarisasi

Standardisasi adalah teknik transformasi yang paling umum digunakan dalam tahap persiapan pemodelan. Untuk fitur numerik, kita tidak akan melakukan transformasi dengan one-hot-encoding seperti pada fitur kategori. Kita akan menggunakan teknik StandarScaler dari library Scikitlearn.

StandardScaler melakukan proses standarisasi fitur dengan mengurangkan mean (nilai rata-rata) kemudian membaginya dengan standar deviasi untuk menggeser distribusi.  StandardScaler menghasilkan distribusi dengan standar deviasi sama dengan 1 dan mean sama dengan 0. Sekitar 68% dari nilai akan berada di antara -1 dan 1.

Pada kasus ini kita hanya akan melakukan standarisai pada data latih dan data uji.
"""

# Standarisasi data latih (train) dengan StandardCaler
scaler = StandardScaler()
X_train[:] = scaler.fit_transform(X_train[:])
X_train.head()

# mengecek nilai mean dan standar deviasi pada setelah proses standarisasi
X_train.describe().round(4)

"""Seperti yang disebutkan sebelumnya, proses ini akan mengubah nilai rata-rata (mean) menjadi 0 dan standar deviasi menjadi 1."""

# Standarisasi data latih (test) dengan StandardCaler
X_test[:] = scaler.fit_transform(X_test[:])
X_test.head()

# mengecek nilai mean dan standar deviasi pada setelah proses standarisasi
X_test.describe().round(4)

"""# Model Deployment

Pada tahap permodelan ini saya akan menggunakan 3 model yang berbeda, berikut ini adalah ketiga algoritma tersebut:
1. K-Nearest Neighbor (KNN)
  - Kelebihan
    - Sederhana dan mudah diimplementasikan
    - Non-parametrik
    - Fleksibel
  - Kekurangan
    - Lambat pada data besar
    - Sensitif terhadap fitur skala
    - Rentan terhadap outlier

2. Random Forest (RF)
  - Kelebihan
    - Akurasi tinggi
    - Resisten terhadap overfitting
    - Dapat menangani data yang hilang dan fitur penting
  - Kekurangan
    - Kurang interpretatif
    - Lambat dalam prediksi

3. Boosting Algorithm
  - Kelebihan
    - Akurasi sangat tinggi
    - Bagus untuk data tidak seimbang
    - Mengurangi bias
  - Kekurangan
    - Lebih rentan terhadap overfitting
    - Waktu pelatihan yang lama
    - Memerlukan tuning parameter

Pada tahap ini kita hanya melatih data training dan menyimpan data testing dari semua model untuk tahap evaluasi yang akan dibahas di Modul Evaluasi Model

## Model K-Nearest Neighbor (K-NN)
"""

knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(X_train, y_train)
knn_predictions = knn.predict(X_test)

"""pada tahapan ini kita akan melatih data dengan KNN, kita menggunakan `n_neighbors`= 10 tetangga dan metric Euclidean untuk mengukur jarak antara titik.

## Model Random Forest
"""

RF = RandomForestClassifier(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)
RF_predictions = RF.predict(X_test)

"""Berikut adalah parameter-parameter yang digunakan:

- `n_estimator`: jumlah trees (pohon) di forest. Di sini kita set `n_estimator`=50.
- `max_depth`: ukuran seberapa banyak pohon dapat membelah (splitting) untuk membagi setiap node ke dalam jumlah pengamatan yang diinginkan. Di sini kita set `max_depth`=16.
- `random_state`: digunakan untuk mengontrol random number generator yang digunakan. Di sini kita set `random_state`=55.
- `n_jobs`:  komponen untuk mengontrol thread atau proses yang berjalan secara paralel.  Di sini kita set `n_job`s=-1 artinya semua proses berjalan secara paralel.

## Model Boosting Algorithm
"""

boosting = AdaBoostClassifier(learning_rate=0.05, random_state=55)
boosting.fit(X_train, y_train)
boosting_predictions = boosting.predict(X_test)

"""kita akan menggunakan metode adaptive boosting. Salah satu metode adaptive boosting yang terkenal adalah AdaBoost.

Berikut merupakan parameter-parameter yang digunakan pada potongan kode di atas.

- `learning_rate`: bobot yang diterapkan pada setiap regressor di masing-masing proses iterasi boosting.
- `random_state`: digunakan untuk mengontrol random number generator yang digunakan.

# Evaluasi Model

Pada proses evaluasi kita akan menggunakan metrik Accuracy, Precision, Recall dan F1-Score untuk menentukan peforma mana yang terbaik

Selanjutnya adalah melakukan evaluasi pada ketiga model.
"""

evaluasi = pd.DataFrame(columns=['train', 'test'], index = ['KNN', 'RF', 'Boosting'])

model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}

for name, model in model_dict.items():
    evaluasi.loc[name, 'train'] = metrics.accuracy_score(y_true=y_train, y_pred=model.predict(X_train))
    evaluasi.loc[name, 'test'] = metrics.accuracy_score(y_true=y_test, y_pred=model.predict(X_test))

evaluasi = evaluasi.applymap(lambda x: "{:.2f}%".format(x * 100))
print(evaluasi)

y_true = y_test

models = {
    'KNN': knn_predictions,
    'Random Forest': RF_predictions,
    'Boosting': boosting_predictions
}
# Define a function to calculate metrics
def calculate_metrics(y_true, y_pred):
    accuracy = metrics.accuracy_score(y_true, y_pred)
    precision = metrics.precision_score(y_true, y_pred, average='weighted')
    recall = metrics.recall_score(y_true, y_pred, average='weighted')
    f1 = metrics.f1_score(y_true, y_pred, average='weighted')
    return {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}

# Calculate metrics for each model
model_metrics = {model: calculate_metrics(y_true, predictions) for model, predictions in models.items()}

colors = ['blue', 'green', 'red']

# Create a figure with subplots
fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(7, 15))
fig.suptitle('Classification Metrics for KNN, SVM, and Random Forest', fontsize=16)

# Metrics to plot
metrics_list = ['Accuracy', 'Precision', 'Recall', 'F1 Score']

# Plotting each metric in its own subplot
for i, metric in enumerate(metrics_list):
    values = [model_metrics[model][metric] for model in models]
    sns.barplot(x=values, y=list(models.keys()), ax=axes[i], palette=colors)
    axes[i].set_title(metric)
    axes[i].set_xlabel('Score')

# Adjust layout for better spacing
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""Selanjutnya kita akan melihat nilai akurasi di tiap model"""

# Calculate metrics for each model
model_metrics = {model: calculate_metrics(y_true, predictions) for model, predictions in models.items()}

# Print the metrics for each model
for model, metrics in model_metrics.items():
    print(f"Model: {model}")
    for metric_name, metric_value in metrics.items():
        print(f"{metric_name}: {metric_value:.4f}")
    print("-" * 20)

"""Berdasarkan visualisasi dan nilai data diatas, terlihat bahwa model `Random Forest` memiliki performa terbaik daripada model `K-Nearest Neighbors` dan
`Boosting Algorithm` pada `Accuracy` dengan nilai 94,78%, `Precision` dengan
nilai 94,8%, `Recall` dengan nilai 94,78%, dan `F1 Score` dengan nilai 94,76%.

Selanjutnya kita uji prediksinya menggunakan beberapa nilai dalam data
"""

# Dictionary untuk mapping angka ke label cuaca
weather_mapping = {0: 'Cloudy', 1: 'Rainy', 2: 'Snowy', 3: 'Sunny'}

# Pilih data prediksi secara acak
prediksi = X_test.sample(n=1, random_state=None).copy()  # n=1 untuk mengambil 1 baris secara acak

# Ambil y_test yang sesuai dengan indeks yang dipilih
y_true_sample = y_test.loc[prediksi.index]

# Dictionary untuk menyimpan hasil prediksi dan nilai asli
pred_dict = {'y_true': y_true_sample}

# Loop melalui setiap model dan prediksi
for name, model in model_dict.items():
    # Prediksi hasil
    pred = model.predict(prediksi).round().astype(int)  # Bulatkan dan ubah ke integer
    # Ubah angka prediksi menjadi nama cuaca
    pred_dict['prediksi_' + name] = [weather_mapping.get(p, 'Unknown') for p in pred]

# Ubah y_true menjadi nama cuaca juga
pred_dict['y_true'] = [weather_mapping.get(y, 'Unknown') for y in y_true_sample.round().astype(int)]

# Tampilkan hasil dalam DataFrame
pd.DataFrame(pred_dict)

"""Berdasarkan prediksinya juga, `Random forest` memiliki hasil prediksi terbaik"""